{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import re\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from nltk.corpus import stopwords\n",
    "#stop_words = set(stopwords.words('english'))\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import PassiveAggressiveClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "import seaborn as sns\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize.treebank import TreebankWordDetokenizer\n",
    "import unidecode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords_cust = stopwords.words('portuguese')\n",
    "\n",
    "df_swc = pd.read_excel('C:\\Users\\marcel.massa.LGE\\ML\\stop_words_customized.xlsx')\n",
    "\n",
    "# STOP_WORDS_CUSTOMIZED is the name of the column in the Excel file\n",
    "\n",
    "df_swc.STOP_WORDS_CUSTOMIZED = df_swc.STOP_WORDS_CUSTOMIZED.str.normalize('NFKD').str.encode('ascii', errors='ignore').str.decode('utf-8').str.lower()\n",
    "for count in range (df_swc.shape[0]):\n",
    "    stopwords_cust.append(df_swc.STOP_WORDS_CUSTOMIZED[count])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Load Training data\n",
    "\n",
    "df = pd.read_excel('C:\\\\Users\\\\marcel.massa.LGE\\\\ML\\\\train_multilabel_NEW.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRAINING DATA ANALYSIS SECTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Number of comments in each category\n",
    "\n",
    "df_symptoms = df.drop(['Claim'], axis=1)\n",
    "counts = []\n",
    "categories = list(df_symptoms.columns.values)\n",
    "for i in categories:\n",
    "    counts.append((i, df_symptoms[i].sum()))\n",
    "df_stats = pd.DataFrame(counts, columns=['category', 'number_of_claims'])\n",
    "df_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# same as above, but graphically\n",
    "\n",
    "df_stats.plot(x='category', y='number_of_claims', kind='bar', legend=False, grid=True, figsize=(15, 5))\n",
    "plt.title(\"Number of claims per category\")\n",
    "plt.ylabel('# of Occurrences', fontsize=12)\n",
    "plt.xlabel('category', fontsize=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Number of characters on the text claims\n",
    "\n",
    "lens = df.Claim.str.len()\n",
    "lens.hist(bins = np.arange(0,5000,50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#How many comments have multiple labels?\n",
    "\n",
    "rowsums = df.iloc[:,2:].sum(axis=1)\n",
    "x=rowsums.value_counts()\n",
    "\n",
    "#plot\n",
    "plt.figure(figsize=(8,5))\n",
    "ax = sns.barplot(x.index, x.values)\n",
    "plt.title(\"Multiple categories per claim\")\n",
    "plt.ylabel('# of Occurrences', fontsize=12)\n",
    "#plt.xlabel('# of categories', fontsize=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Just checking\n",
    "\n",
    "df['Claim'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#categories = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
    "\n",
    "#Just checking\n",
    "categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into train and test sets and prepare for vectorization\n",
    "\n",
    "train, test = train_test_split(df, random_state=42, test_size=0.33, shuffle=True)\n",
    "\n",
    "X_train = train.Claim\n",
    "X_test = test.Claim\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setupt the vectorizer and used it on the splitted training data\n",
    "# Fit and transform steps are separated, God knwos why...\n",
    "\n",
    "vectorizer = TfidfVectorizer(sublinear_tf=True, min_df=5, encoding='utf-8', strip_accents='unicode', ngram_range=(1,2), norm='l2', lowercase=True, stop_words=stopwords_cust)\n",
    "\n",
    "vectorizer.fit(train.Claim)\n",
    "x_train = vectorizer.transform(train.Claim)\n",
    "y_train = train.drop(labels = ['Claim'], axis=1)\n",
    "\n",
    "#vectorizer.fit(test.Claim)\n",
    "x_test = vectorizer.transform(test.Claim)\n",
    "y_test = test.drop(labels = ['Claim'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# using classifier chains\n",
    "from skmultilearn.problem_transform import ClassifierChain\n",
    "\n",
    "# initialize classifier chains multi-label classifier\n",
    "classifier = ClassifierChain(LinearSVC())\n",
    "\n",
    "# Training LinearSVC model on train data\n",
    "classifier.fit(x_train, y_train)\n",
    "\n",
    "# predict\n",
    "predictions = classifier.predict(x_test)\n",
    "\n",
    "# accuracy\n",
    "print(\"Accuracy = \",accuracy_score(y_test,predictions))\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "predictions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_new = pd.read_excel('C:\\Users\\marcel.massa.LGE\\ML\\CIC Call Receiving Rate_120515012.xls', header=1)\n",
    "df_new.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "example = word_tokenize(df_new.loc[0 ,\"Consultation Content\"].lower())\n",
    "example_clean = [w for w in example if not w in stopwords_cust]\n",
    "\n",
    "print (example)\n",
    "print (example_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize.treebank import TreebankWordDetokenizer\n",
    "TreebankWordDetokenizer().detokenize(example_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_new['Texto_Limpo'] = df_new.loc[: ,\"Consultation Content\"]\n",
    "df_new.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for j in range (len(df_new)): \n",
    "    tokens = word_tokenize(unidecode.unidecode(df_new.loc[j ,\"Consultation Content\"].lower()))\n",
    "    clean_tokens = [w for w in tokens if not w in stopwords_cust]\n",
    "    df_new.loc[j, 'Texto_Limpo'] = TreebankWordDetokenizer().detokenize(clean_tokens)\n",
    "    if j == len(df_new)/2:\n",
    "        print('... 50%'),\n",
    "print('... [DONE]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_new.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_new.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(sublinear_tf=True, min_df=5, encoding='utf-8', strip_accents='unicode', ngram_range=(1,2), norm='l2', lowercase=True, stop_words=stopwords_cust)\n",
    "\n",
    "features = vectorizer.fit_transform(df.Claim)\n",
    "print(features.shape)\n",
    "\n",
    "features_new = vectorizer.transform(df_new.loc[: ,\"Consultation Content\"])\n",
    "print(features_new.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import chi2\n",
    "#import numpy as np\n",
    "\n",
    "N = 5\n",
    "for i in categories:\n",
    "    features_chi2 = chi2(features, df[i] )\n",
    "    indices = np.argsort(features_chi2[0])\n",
    "    feature_names = np.array(vectorizer.get_feature_names())[indices]\n",
    "    unigrams = [v for v in feature_names if len(v.split(' ')) == 1]\n",
    "    bigrams = [v for v in feature_names if len(v.split(' ')) == 2]\n",
    "    print(\"# '{}':\".format(i))\n",
    "    print(\"  . Most correlated unigrams:\\n       . {}\".format('\\n       . '.join(unigrams[-N:]).encode('utf-8')))\n",
    "    print(\"  . Most correlated bigrams:\\n       . {}\".format('\\n       . '.join(bigrams[-N:]).encode('utf-8')))\n",
    "\n",
    "#for Sintoma, category_id in sorted(category_to_id.items()):\n",
    "#  features_chi2 = chi2(features, labels == category_id)\n",
    "#  indices = np.argsort(features_chi2[0])\n",
    "#  feature_names = np.array(tfidf.get_feature_names())[indices]\n",
    "#  unigrams = [v for v in feature_names if len(v.split(' ')) == 1]\n",
    "#  bigrams = [v for v in feature_names if len(v.split(' ')) == 2]\n",
    "#  print(\"# '{}':\".format(Sintoma))\n",
    "#  print(\"  . Most correlated unigrams:\\n       . {}\".format('\\n       . '.join(unigrams[-N:]).encode('utf-8')))\n",
    "#  print(\"  . Most correlated bigrams:\\n       . {}\".format('\\n       . '.join(bigrams[-N:]).encode('utf-8')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skmultilearn.problem_transform import ClassifierChain\n",
    "classifier = ClassifierChain(LinearSVC())\n",
    "\n",
    "# Training LinearSVC model on train data\n",
    "classifier.fit(features, df.drop(labels = ['Claim'], axis=1))\n",
    "\n",
    "# predict\n",
    "predictions_new = classifier.predict(features_new).toarray()\n",
    "predictions_new.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "test_DF = pd.DataFrame(predictions_new, columns=categories)\n",
    "test_DF.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_DF = pd.concat([df_new, test_DF], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "final_DF.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_DF.to_excel('C:\\Users\\marcel.massa.LGE\\ML\\CIC Call Receiving Rate_MULTI_LABEL_2_PREDICTED.xlsx')\n",
    "print('\\n\\n*** FINISHED ***')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new = df_new.reindex(columns=df_new.columns.tolist() + categories)   # add empty cols\n",
    "df_new.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for i in range (len(categories)):\n",
    "    print('... Processing {}'.format(categories[i])),\n",
    "    for j in range (predictions_new.shape[0]):\n",
    "        df_new.loc[j, categories[i]] = predictions_new[j,i]\n",
    "        if j == predictions_new.shape[0]/2:\n",
    "            print('... 50%'),\n",
    "    print('... [DONE]')\n",
    "    \n",
    "print('\\n\\n*** FINISHED ***')\n",
    "\n",
    "df_new.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new.to_excel('C:\\Users\\marcel.massa.LGE\\ML\\CIC Call Receiving Rate_MULTI_LABEL_2_PREDICTED_BETA.xlsx')\n",
    "print('\\n\\n*** FINISHED ***')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-\n",
    "-\n",
    "-\n",
    "-\n",
    "-\n",
    "Example for LabelPowerset\n",
    "-\n",
    "-\n",
    "-\n",
    "-\n",
    "-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using Label Powerset\n",
    "from skmultilearn.problem_transform import LabelPowerset\n",
    "\n",
    "# initialize label powerset multi-label classifier\n",
    "classifier = LabelPowerset(LinearSVC())\n",
    "\n",
    "# train\n",
    "classifier.fit(x_train, y_train)\n",
    "\n",
    "# predict\n",
    "predictions = classifier.predict(x_test)\n",
    "\n",
    "# accuracy\n",
    "print(\"Accuracy = \",accuracy_score(y_test,predictions))\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test, predictions, target_names=categories))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
